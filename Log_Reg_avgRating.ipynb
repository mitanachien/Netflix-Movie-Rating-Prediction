{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Log_Reg_avgRating.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfN4N1UPC1QhbMPCoQuGEq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7s3eNjFqLrP4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619547923591,"user_tz":240,"elapsed":52590,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"800ca311-bc8f-4d46-a5e1-24b3ff3134e3"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qQPj8jD-MU5F","executionInfo":{"status":"ok","timestamp":1619547938310,"user_tz":240,"elapsed":67301,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8ud0pJkMyv6","executionInfo":{"status":"ok","timestamp":1619547951788,"user_tz":240,"elapsed":80775,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["!wget -q https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8fEU0oDM53R","executionInfo":{"status":"ok","timestamp":1619547954644,"user_tz":240,"elapsed":83627,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["!tar xf spark-3.1.1-bin-hadoop2.7.tgz"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBSugYSqM8HY","executionInfo":{"status":"ok","timestamp":1619547958688,"user_tz":240,"elapsed":87667,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["!pip install -q findspark"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY9fCocINXzZ","executionInfo":{"status":"ok","timestamp":1619547958689,"user_tz":240,"elapsed":87664,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozxEuDnFNf1H","executionInfo":{"status":"ok","timestamp":1619547958689,"user_tz":240,"elapsed":87661,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["import findspark\n","findspark.init()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"6SkbUTFzNj0x","executionInfo":{"status":"ok","timestamp":1619547958691,"user_tz":240,"elapsed":87657,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"32ef884a-b5ce-48af-f41f-1c3891323d3c"},"source":["findspark.find()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/spark-3.1.1-bin-hadoop2.7'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"AEWVKrXJNqaP","executionInfo":{"status":"ok","timestamp":1619547965359,"user_tz":240,"elapsed":94319,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.master(\"local\").appName('MovieLogReg_avgRating').config('spark.ui.port', '4050').getOrCreate()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k9f6js90zukq"},"source":["**Loading Data**"]},{"cell_type":"code","metadata":{"id":"lrdsQgPbzmnd","executionInfo":{"status":"ok","timestamp":1619548315698,"user_tz":240,"elapsed":444655,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["training_data = spark.read.load(\"/content/gdrive/MyDrive/Colab Notebooks/CSC 522/training_avgRating.csv\",\n","                     format=\"csv\", inferSchema=True, header=True)\n","validation_data = spark.read.load(\"/content/gdrive/MyDrive/Colab Notebooks/CSC 522/validation_avgRating.csv\",\n","                     format=\"csv\", inferSchema=True, header=True)\n","testing_data = spark.read.load(\"/content/gdrive/MyDrive/Colab Notebooks/CSC 522/testing_avgRating.csv\",\n","                     format=\"csv\", inferSchema=True, header=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TiaQyt1XKZ2","executionInfo":{"status":"ok","timestamp":1619548315878,"user_tz":240,"elapsed":444830,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["from pyspark.sql.functions import round\n","train_DATA = training_data.withColumn('avgRating', round('avg(Rating)', 0))\n","vali_DATA = validation_data.withColumn('avgRating', round('avg(Rating)', 0))\n","test_DATA = testing_data.withColumn('avgRating', round('avg(Rating)', 0))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"pDAi50fLTn0V","executionInfo":{"status":"ok","timestamp":1619548315886,"user_tz":240,"elapsed":444833,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"4005c149-eb85-4d7b-cb84-e0b9becd122d"},"source":["'''\n","from pyspark.sql.types import IntegerType\n","train_DATA = train_DATA.withColumn('avgRating',train_DATA['avgRating'].cast(IntegerType()))\n","vali_DATA = vali_DATA.withColumn('avgRating',vali_DATA['avgRating'].cast(IntegerType()))\n","test_DATA = test_DATA.withColumn('avgRating',test_DATA['avgRating'].cast(IntegerType()))\n","'''"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfrom pyspark.sql.types import IntegerType\\ntrain_DATA = train_DATA.withColumn('avgRating',train_DATA['avgRating'].cast(IntegerType()))\\nvali_DATA = vali_DATA.withColumn('avgRating',vali_DATA['avgRating'].cast(IntegerType()))\\ntest_DATA = test_DATA.withColumn('avgRating',test_DATA['avgRating'].cast(IntegerType()))\\n\""]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lq5qnedQU-Y3","executionInfo":{"status":"ok","timestamp":1619548315887,"user_tz":240,"elapsed":444825,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"b1ed534a-09e0-4e91-d0df-00a2888b46c5"},"source":["train_DATA.printSchema()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["root\n"," |-- Actor1Index: double (nullable = true)\n"," |-- Actor3Index: double (nullable = true)\n"," |-- Genre2Index: double (nullable = true)\n"," |-- Genre3Index: double (nullable = true)\n"," |-- LanguageIndex: double (nullable = true)\n"," |-- Writer1Index: double (nullable = true)\n"," |-- YearIndex: double (nullable = true)\n"," |-- avg(Duration): double (nullable = true)\n"," |-- avg(Rating): double (nullable = true)\n"," |-- avgRating: double (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Lsde2wXz6Hk"},"source":["**One-Hot Encoding**"]},{"cell_type":"code","metadata":{"id":"yV5R0bTtbixj","executionInfo":{"status":"ok","timestamp":1619548316048,"user_tz":240,"elapsed":444980,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["from pyspark.ml.feature import (VectorAssembler,VectorIndexer, OneHotEncoder)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"OY6grCTdz4TC","executionInfo":{"status":"ok","timestamp":1619548316185,"user_tz":240,"elapsed":445113,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["# Encoding\n","#customer_encoder = OneHotEncoder(inputCol='CustomerIndex', outputCol='CustomerVec')\n","actors_encoder = OneHotEncoder(inputCols=['Actor1Index', 'Actor3Index'], outputCols=['Actor1Vec', 'Actor3Vec'])\n","#country_encoder = OneHotEncoder(inputCol='CountryIndex', outputCol='CountryVec')\n","#directors_encoder = OneHotEncoder(inputCols=['Director1Index', 'Director2Index'], outputCols=['Director1Vec', 'Director2Vec'])\n","#genre1_encoder = OneHotEncoder(inputCol='Genre1Index', outputCol='Genre1Vec')\n","genres_encoder = OneHotEncoder(inputCols=['Genre2Index','Genre3Index'], outputCols=['Genre2Vec','Genre3Vec'])\n","language_encoder = OneHotEncoder(inputCol='LanguageIndex', outputCol='LanguageVec')\n","#pc_encoder = OneHotEncoder(inputCol='PCIndex', outputCol='PCVec')\n","writers_encoder = OneHotEncoder(inputCol='Writer1Index', outputCol='Writer1Vec')\n","year_encoder = OneHotEncoder(inputCol='YearIndex', outputCol='YearVec')"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hZr266K0v1U","executionInfo":{"status":"ok","timestamp":1619548316186,"user_tz":240,"elapsed":445111,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["assembler_encoder = VectorAssembler(inputCols=['Actor1Vec',\n"," 'Actor3Vec',\n"," 'Genre2Vec',\n"," 'Genre3Vec',\n"," 'LanguageVec',\n"," 'Writer1Vec',\n"," 'YearVec',\n"," 'avg(Duration)'],outputCol='features')"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qNBdDmbWKew"},"source":["**Create the Logistic Regression Model and fit the data**"]},{"cell_type":"code","metadata":{"id":"mJbIlADQd5Dr","executionInfo":{"status":"ok","timestamp":1619548316186,"user_tz":240,"elapsed":445107,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["from pyspark.ml.classification import LogisticRegression"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhOn5cuhcX8S","executionInfo":{"status":"ok","timestamp":1619548316187,"user_tz":240,"elapsed":445104,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["# Hyperparameters\n","iteration = 200\n","regParam = 0.3\n","elasticNetParam = 0.8"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGXIt8RQONkj","executionInfo":{"status":"ok","timestamp":1619548316188,"user_tz":240,"elapsed":445101,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["log_reg_movie = LogisticRegression(featuresCol='features', labelCol='avgRating', maxIter=iteration, regParam=regParam, elasticNetParam=elasticNetParam)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"oR2wyUQNz4o5","executionInfo":{"status":"ok","timestamp":1619548316189,"user_tz":240,"elapsed":445099,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["# Setting ML Pipeline\n","from pyspark.ml import Pipeline\n","pipeline = Pipeline(stages=[actors_encoder, \n","                            genres_encoder, \n","                            language_encoder, \n","                            writers_encoder, \n","                            year_encoder,\n","                            assembler_encoder,\n","                            log_reg_movie])"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JB38DT3c_aK","executionInfo":{"status":"ok","timestamp":1619548355153,"user_tz":240,"elapsed":484059,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["# Fit the data\n","log_reg_model = pipeline.fit(train_DATA)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"haJIhzICOTv-","executionInfo":{"status":"ok","timestamp":1619548355663,"user_tz":240,"elapsed":484564,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["# Validation Prediction\n","vali_pred = log_reg_model.transform(vali_DATA)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeOpskCqhDvo","executionInfo":{"status":"ok","timestamp":1619548356105,"user_tz":240,"elapsed":485001,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"391cf5e0-0b9b-46a2-9f4d-33decb9428c6"},"source":["vali_pred.select('prediction', 'avgRating').show()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["+----------+---------+\n","|prediction|avgRating|\n","+----------+---------+\n","|       3.0|      4.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      4.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      2.0|\n","|       3.0|      2.0|\n","|       3.0|      4.0|\n","|       3.0|      4.0|\n","|       3.0|      3.0|\n","|       3.0|      2.0|\n","|       3.0|      3.0|\n","|       3.0|      4.0|\n","|       3.0|      3.0|\n","|       3.0|      3.0|\n","|       3.0|      2.0|\n","+----------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wpDVPK5vdlpS"},"source":["**Evaluating The Results**"]},{"cell_type":"code","metadata":{"id":"MVAU_BCVWIXA","executionInfo":{"status":"ok","timestamp":1619548356106,"user_tz":240,"elapsed":484997,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["from pyspark.ml.evaluation import (MulticlassClassificationEvaluator, RegressionEvaluator, RankingEvaluator)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"_P7oRZrDd2Za","executionInfo":{"status":"ok","timestamp":1619548356107,"user_tz":240,"elapsed":484993,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["my_f1_eval = MulticlassClassificationEvaluator(labelCol='avgRating')\n","my_rmse_eval = RegressionEvaluator(labelCol='avgRating')"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"MC31osPaiaH4","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1619548357608,"user_tz":240,"elapsed":486486,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}},"outputId":"2481bf2d-502f-4715-99ac-1d69252bd4ef"},"source":["#f1 = my_f1_eval.evaluate(vali_pred)\n","rmse = my_rmse_eval.evaluate(vali_pred)"],"execution_count":26,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-53cbe4665c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#f1 = my_f1_eval.evaluate(vali_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_rmse_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvali_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o508.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 180.0 failed 1 times, most recent failure: Lost task 1.0 in stage 180.0 (TID 1349) (19e550972e07 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderModel$$Lambda$2804/632909055: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen value: 21.0. To handle unseen values, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:70)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:62)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:106)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:115)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:101)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderModel$$Lambda$2804/632909055: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen value: 21.0. To handle unseen values, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\n\t... 36 more\n"]}]},{"cell_type":"code","metadata":{"id":"zUoWuNIuipXS","executionInfo":{"status":"aborted","timestamp":1619548357462,"user_tz":240,"elapsed":486335,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NAEbT1E77B9","executionInfo":{"status":"aborted","timestamp":1619548357464,"user_tz":240,"elapsed":486333,"user":{"displayName":"Chi-Jen Chien","photoUrl":"","userId":"06662732836002486797"}}},"source":["rmse"],"execution_count":null,"outputs":[]}]}